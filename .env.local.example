# LLM Configuration
NEXT_PUBLIC_LLM_PROVIDER=azure  # can be 'github' or 'azure'

# GitHub Models Configuration
NEXT_PUBLIC_LLM_BASE_URL=https://models.inference.ai.azure.com
NEXT_PUBLIC_LLM_API_KEY=your_github_pat_here
NEXT_PUBLIC_LLM_MODEL=gpt-4o-mini

# Azure OpenAI Configuration
NEXT_PUBLIC_AZURE_OPENAI_ENDPOINT=https://your-deployment.openai.azure.com
NEXT_PUBLIC_AZURE_OPENAI_KEY=your_azure_openai_key_here
NEXT_PUBLIC_AZURE_OPENAI_MODEL=gpt-4o-mini

# Bland AI Configuration
NEXT_PUBLIC_BLAND_API_KEY=your_bland_api_key_here

# API Endpoints
RAG_API_ENDPOINT=your_rag_endpoint_here
ORDER_API_ENDPOINT=your_order_endpoint_here

# App Configuration
NEXT_PUBLIC_APP_URL=http://localhost:3000

# Note: Never commit the actual .env.local file to version control
# This is just an example file. Copy it to .env.local and fill in your actual values.

# Provider Configuration Notes:
# 1. For GitHub Models:
#    - Get your GitHub PAT from: https://github.com/settings/tokens
#    - Ensure it has the necessary model access permissions
#
# 2. For Azure OpenAI:
#    - Get your endpoint and key from Azure OpenAI service
#    - Use your deployment name as the model name
#
# 3. For Bland AI:
#    - Get your API key from: https://www.bland.ai
#    - Used for phone call functionality